# Retrieval Augmented Generation (RAG)
- An AI framework that combines the strengths of traditional information retrieval systems (such as search and databases) with the capabilities of generative large language models (LLMs).
- By combining your data and world knowledge with LLM, generated outputs are more accurate, up-to-date, and relevant to your specific needs.

### How does Retrieval-Augmented Generation work?

1. Retrieval and Pre-processing: 
- RAGs leverage powerful search algorithms to query external data, such as web pages, knowledge bases, and databases.
- Once retrieved, the relevant information undergoes pre-processing, including tokenization, stemming, and removal of stop words.

2. Generation: 
- The pre-processed retrieved information is then seamlessly incorporated into the pre-trained LLM.
- This integration enhances the LLM's context, providing it with a more comprehensive understanding of the topic.
- This augmented context enables the LLM to generate more precise, informative, and engaging responses. 

### Behind the scene:
- RAG operates by first retrieving relevant information from a database using a query generated by the LLM. 
- This retrieved information is then integrated into the LLM's query input, enabling it to generate more accurate and contextually relevant text. 
- Retrieval is usually handled by a semantic search engine that uses embeddings stored in vector databases.  
- Sophisticated ranking and query rewriting features, ensuring that the results are relevant to the query and will answer the userâ€™s question.

### Why use RAG?
- LLMs are limited to their pre-trained data, this leads to outdated responses.
- RAG overcomes this by providing up-to-date information to LLMs.
- LLMs are powerful tools for generating creative and engaging text, but they can sometimes struggle with factual accuracy.
- This is because LLMs are trained on massive amounts of text data, which may contain inaccuracies or biases.
- RAGs usually retrieve facts via search, and modern search engines now leverage vector databases to efficiently retrieve relevant documents.
- Vector databases store documents as embeddings in a high-dimensional space, allowing for fast and accurate retrieval based on semantic similarity.
- Multi-modal embeddings can be used for images, audio and video, and more and these media embeddings can be retrieved alongside text embeddings or multi-language embeddings.
- Advanced search engines like Vertex AI Search use semantic search and keyword search together (called hybrid search)
- A re-ranker which scores search results to ensure the top returned results are the most relevant.
- Additionally searches perform better with a clear, focused query without misspellings, so prior to lookup, sophisticated search engines will transform a query and fix spelling mistakes.
- RAG helps to minimize contradictions and inconsistencies in the generated text.
- sThis significantly improves the quality of the generated text, and improves the user experience.
