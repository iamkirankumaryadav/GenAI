### **BERT**  
- A model for NLP developed by **Google** that learns bi-directional representations of text.
- Significantly improve contextual understanding of unlabeled text across many different tasks.
- It's the basis for an entire family of BERT-like models such as RoBERTa, ALBERT, and DistilBERT.

### **RoBERTa: A Robust BERT Variant**
- RoBERTa (Robustly Optimized BERT Approach)
- A powerful language model that builds upon the foundation of BERT (Bidirectional Encoder Representations from Transformers).
- While BERT is a groundbreaking model, RoBERTa enhances it through several key modifications.

**RoBERTa's role:**
- **Tokenization:** The review is broken down into tokens.
- **Embedding:** Each token is converted into a numerical representation.
- **Encoding:** The model processes the sequence of embeddings using its transformer architecture to capture contextual information.
- **Sentiment Prediction:** RoBERTa predicts the sentiment of the review.

### **T5 Model:**  
- An encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format.

Llama (large language model (LLM) developed by Meta AI 
- It stands for Large Language Model Meta AI, and it's built on the transformer architecture.
- It's designed to understand and generate human-like text.
